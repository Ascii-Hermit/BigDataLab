{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e42b696f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 2.1.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/lib/python3/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/lib/python3/dist-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\", line 461, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\", line 450, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\", line 357, in dispatch_shell\n",
      "    await result\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\", line 652, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/lib/python3/dist-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 2914, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_16474/3940490146.py\", line 5, in <module>\n",
      "    from pyspark.ml.feature import Imputer, StandardScaler\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/ml/__init__.py\", line 22, in <module>\n",
      "    from pyspark.ml.base import (\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/ml/base.py\", line 40, in <module>\n",
      "    from pyspark.ml.param import P\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/ml/param/__init__.py\", line 35, in <module>\n",
      "    from pyspark.ml.linalg import DenseVector, Vector, Matrix\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/ml/linalg/__init__.py\", line 81, in <module>\n",
      "    import scipy.sparse\n",
      "  File \"/usr/lib/python3/dist-packages/scipy/sparse/__init__.py\", line 267, in <module>\n",
      "    from ._csr import *\n",
      "  File \"/usr/lib/python3/dist-packages/scipy/sparse/_csr.py\", line 10, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/07 10:29:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'createDat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16474/3940490146.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"feature1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"feature2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"feature3\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initial DataFrame:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'createDat'"
     ]
    }
   ],
   "source": [
    "# Implement a PySpark script to handle any missing values and scale numerical features.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import Imputer, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HandleMissingValuesAndScale\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (1.0, 0.1, None),\n",
    "    (2.0, None, 0.2),\n",
    "    (None, 0.3, 0.3),\n",
    "    (4.0, 0.4, None),\n",
    "    (5.0, 0.5, 0.5)\n",
    "]\n",
    "\n",
    "columns = [\"feature1\", \"feature2\", \"feature3\"]\n",
    "df = spark.createDat\n",
    "print(\"Initial DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=[\"feature1\", \"feature2\", \"feature3\"],\n",
    "    outputCols=[\"feature1_imputed\", \"feature2_imputed\", \"feature3_imputed\"]\n",
    ")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"feature1_imputed\", \"feature2_imputed\", \"feature3_imputed\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaled_features\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[imputer, assembler, scaler])\n",
    "\n",
    "# fit and transform the pipeline\n",
    "pipeline_model = pipeline.fit(df)\n",
    "scaled_df = pipeline_model.transform(df)\n",
    "\n",
    "print(\"DataFrame after handling missing values and scaling:\")\n",
    "scaled_df.select(\"feature1\", \"feature2\", \"feature3\", \"scaled_features\").show(truncate=False)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "378c55b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/07 10:29:39 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial DataFrame:\n",
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|0.0|0.0|\n",
      "|0.1|0.1|\n",
      "|0.2|0.2|\n",
      "|9.0|9.0|\n",
      "|9.1|9.1|\n",
      "|9.2|9.2|\n",
      "+---+---+\n",
      "\n",
      "+---+---+---------+\n",
      "|  x|  y| features|\n",
      "+---+---+---------+\n",
      "|0.0|0.0|(2,[],[])|\n",
      "|0.1|0.1|[0.1,0.1]|\n",
      "|0.2|0.2|[0.2,0.2]|\n",
      "|9.0|9.0|[9.0,9.0]|\n",
      "|9.1|9.1|[9.1,9.1]|\n",
      "|9.2|9.2|[9.2,9.2]|\n",
      "+---+---+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/07 10:29:43 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers:\n",
      "[9.1 9.1]\n",
      "[0.1 0.1]\n",
      "Predictions:\n",
      "+---+---+----------+\n",
      "|  x|  y|prediction|\n",
      "+---+---+----------+\n",
      "|0.0|0.0|         1|\n",
      "|0.1|0.1|         1|\n",
      "|0.2|0.2|         1|\n",
      "|9.0|9.0|         0|\n",
      "|9.1|9.1|         0|\n",
      "|9.2|9.2|         0|\n",
      "+---+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Develop a PySpark script that uses the K-means algorithm to cluster data points.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KMeansClustering\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (0.0, 0.0),\n",
    "    (0.1, 0.1),\n",
    "    (0.2, 0.2),\n",
    "    (9.0, 9.0),\n",
    "    (9.1, 9.1),\n",
    "    (9.2, 9.2)\n",
    "]\n",
    "\n",
    "columns = [\"x\", \"y\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Initial DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"x\", \"y\"], outputCol=\"features\")\n",
    "\n",
    "feature_df = assembler.transform(df)\n",
    "feature_df.show()\n",
    "\n",
    "kmeans = KMeans(k=2, seed=1)  # change k and see what happens\n",
    "model = kmeans.fit(feature_df)\n",
    "\n",
    "predictions = model.transform(feature_df)\n",
    "\n",
    "print(\"Cluster Centers:\")\n",
    "for center in model.clusterCenters():\n",
    "    print(center)\n",
    "\n",
    "print(\"Predictions:\")\n",
    "predictions.select(\"x\", \"y\", \"prediction\").show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa14590e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial DataFrame:\n",
      "+----+----+\n",
      "|   x|   y|\n",
      "+----+----+\n",
      "| 0.0| 0.0|\n",
      "| 0.1| 0.1|\n",
      "| 0.2| 0.2|\n",
      "| 9.0| 9.0|\n",
      "| 9.1| 9.1|\n",
      "| 9.2| 9.2|\n",
      "|11.0|11.0|\n",
      "+----+----+\n",
      "\n",
      "Labeled Predictions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 2.1.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 152, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1377, in _parse_datatype_json_string\n",
      "    return _parse_datatype_json_value(json.loads(json_string))\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1419, in _parse_datatype_json_value\n",
      "    return _all_complex_types[tpe].fromJson(json_value)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1017, in fromJson\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1017, in <listcomp>\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 708, in fromJson\n",
      "    _parse_datatype_json_value(json[\"type\"]),\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1421, in _parse_datatype_json_value\n",
      "    return UserDefinedType.fromJson(json_value)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1205, in fromJson\n",
      "    m = __import__(pyModule, globals(), locals(), [pyClass])\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n",
      "    from pyspark.ml.base import (\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 40, in <module>\n",
      "    from pyspark.ml.param import P\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 35, in <module>\n",
      "    from pyspark.ml.linalg import DenseVector, Vector, Matrix\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 81, in <module>\n",
      "    import scipy.sparse\n",
      "  File \"/usr/lib/python3/dist-packages/scipy/sparse/__init__.py\", line 267, in <module>\n",
      "    from ._csr import *\n",
      "  File \"/usr/lib/python3/dist-packages/scipy/sparse/_csr.py\", line 10, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 2.1.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 152, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1377, in _parse_datatype_json_string\n",
      "    return _parse_datatype_json_value(json.loads(json_string))\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1419, in _parse_datatype_json_value\n",
      "    return _all_complex_types[tpe].fromJson(json_value)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1017, in fromJson\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1017, in <listcomp>\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 708, in fromJson\n",
      "    _parse_datatype_json_value(json[\"type\"]),\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1421, in _parse_datatype_json_value\n",
      "    return UserDefinedType.fromJson(json_value)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1205, in fromJson\n",
      "    m = __import__(pyModule, globals(), locals(), [pyClass])\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n",
      "    from pyspark.ml.base import (\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 40, in <module>\n",
      "    from pyspark.ml.param import P\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 35, in <module>\n",
      "    from pyspark.ml.linalg import DenseVector, Vector, Matrix\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 81, in <module>\n",
      "    import scipy.sparse\n",
      "  File \"/usr/lib/python3/dist-packages/scipy/sparse/__init__.py\", line 267, in <module>\n",
      "    from ._csr import *\n",
      "  File \"/usr/lib/python3/dist-packages/scipy/sparse/_csr.py\", line 10, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AttributeError: _ARRAY_API not found\n",
      "AttributeError: _ARRAY_API not found\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 2.1.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 152, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1377, in _parse_datatype_json_string\n",
      "    return _parse_datatype_json_value(json.loads(json_string))\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1419, in _parse_datatype_json_value\n",
      "    return _all_complex_types[tpe].fromJson(json_value)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1017, in fromJson\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1017, in <listcomp>\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 708, in fromJson\n",
      "    _parse_datatype_json_value(json[\"type\"]),\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1421, in _parse_datatype_json_value\n",
      "    return UserDefinedType.fromJson(json_value)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1205, in fromJson\n",
      "    m = __import__(pyModule, globals(), locals(), [pyClass])\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n",
      "    from pyspark.ml.base import (\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 40, in <module>\n",
      "    from pyspark.ml.param import P\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 35, in <module>\n",
      "    from pyspark.ml.linalg import DenseVector, Vector, Matrix\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 81, in <module>\n",
      "    import scipy.sparse\n",
      "  File \"/usr/lib/python3/dist-packages/scipy/sparse/__init__.py\", line 267, in <module>\n",
      "    from ._csr import *\n",
      "  File \"/usr/lib/python3/dist-packages/scipy/sparse/_csr.py\", line 10, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 2.1.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 2.1.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 152, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1377, in _parse_datatype_json_string\n",
      "    return _parse_datatype_json_value(json.loads(json_string))\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1419, in _parse_datatype_json_value\n",
      "    return _all_complex_types[tpe].fromJson(json_value)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1017, in fromJson\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1017, in <listcomp>\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 708, in fromJson\n",
      "    _parse_datatype_json_value(json[\"type\"]),\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1421, in _parse_datatype_json_value\n",
      "    return UserDefinedType.fromJson(json_value)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1205, in fromJson\n",
      "    m = __import__(pyModule, globals(), locals(), [pyClass])\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n",
      "    from pyspark.ml.base import (\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 40, in <module>\n",
      "    from pyspark.ml.param import P\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 35, in <module>\n",
      "    from pyspark.ml.linalg import DenseVector, Vector, Matrix\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 81, in <module>\n",
      "    import scipy.sparse\n",
      "  File \"/usr/lib/python3/dist-packages/scipy/sparse/__init__.py\", line 267, in <module>\n",
      "    from ._csr import *\n",
      "  File \"/usr/lib/python3/dist-packages/scipy/sparse/_csr.py\", line 10, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 2.1.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 152, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1377, in _parse_datatype_json_string\n",
      "    return _parse_datatype_json_value(json.loads(json_string))\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1419, in _parse_datatype_json_value\n",
      "    return _all_complex_types[tpe].fromJson(json_value)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1017, in fromJson\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1017, in <listcomp>\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 708, in fromJson\n",
      "    _parse_datatype_json_value(json[\"type\"]),\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1421, in _parse_datatype_json_value\n",
      "    return UserDefinedType.fromJson(json_value)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1205, in fromJson\n",
      "    m = __import__(pyModule, globals(), locals(), [pyClass])\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n",
      "    from pyspark.ml.base import (\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 40, in <module>\n",
      "    from pyspark.ml.param import P\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 35, in <module>\n",
      "    from pyspark.ml.linalg import DenseVector, Vector, Matrix\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 81, in <module>\n",
      "    import scipy.sparse\n",
      "  File \"/usr/lib/python3/dist-packages/scipy/sparse/__init__.py\", line 267, in <module>\n",
      "    from ._csr import *\n",
      "  File \"/usr/lib/python3/dist-packages/scipy/sparse/_csr.py\", line 10, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 152, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1377, in _parse_datatype_json_string\n",
      "    return _parse_datatype_json_value(json.loads(json_string))\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1419, in _parse_datatype_json_value\n",
      "    return _all_complex_types[tpe].fromJson(json_value)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1017, in fromJson\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1017, in <listcomp>\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 708, in fromJson\n",
      "    _parse_datatype_json_value(json[\"type\"]),\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1421, in _parse_datatype_json_value\n",
      "    return UserDefinedType.fromJson(json_value)\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1205, in fromJson\n",
      "    m = __import__(pyModule, globals(), locals(), [pyClass])\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n",
      "    from pyspark.ml.base import (\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 40, in <module>\n",
      "    from pyspark.ml.param import P\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 35, in <module>\n",
      "    from pyspark.ml.linalg import DenseVector, Vector, Matrix\n",
      "  File \"/home/lplab/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 81, in <module>\n",
      "    import scipy.sparse\n",
      "  File \"/usr/lib/python3/dist-packages/scipy/sparse/__init__.py\", line 267, in <module>\n",
      "    from ._csr import *\n",
      "  File \"/usr/lib/python3/dist-packages/scipy/sparse/_csr.py\", line 10, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n",
      "AttributeError: _ARRAY_API not found\n",
      "AttributeError: _ARRAY_API not found\n",
      "AttributeErrorAttributeError: : _ARRAY_API not found_ARRAY_API not found\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:========================>                                 (3 + 4) / 7]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+-------------+\n",
      "|   x|   y|prediction|anomaly_label|\n",
      "+----+----+----------+-------------+\n",
      "| 0.0| 0.0|         1|       normal|\n",
      "| 0.1| 0.1|         1|       normal|\n",
      "| 0.2| 0.2|         1|       normal|\n",
      "| 9.0| 9.0|         0|       normal|\n",
      "| 9.1| 9.1|         0|       normal|\n",
      "| 9.2| 9.2|         0|       normal|\n",
      "|11.0|11.0|         0|      anomaly|\n",
      "+----+----+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Develop a PySpark script that labels data points as anomalies based on their cluster\n",
    "# assignments\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AnomalyDetection\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (0.0, 0.0),\n",
    "    (0.1, 0.1),\n",
    "    (0.2, 0.2),\n",
    "    (9.0, 9.0),\n",
    "    (9.1, 9.1),\n",
    "    (9.2, 9.2),\n",
    "    (11.0, 11.0)  # outlier\n",
    "]\n",
    "\n",
    "columns = [\"x\", \"y\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Initial DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"x\", \"y\"], outputCol=\"features\")\n",
    "feature_df = assembler.transform(df)\n",
    "\n",
    "kmeans = KMeans(k=2, seed=1)\n",
    "model = kmeans.fit(feature_df)\n",
    "\n",
    "predictions = model.transform(feature_df)\n",
    "\n",
    "centers = model.clusterCenters() # this is how you get the centers\n",
    "\n",
    "def label_anomaly(features, prediction):\n",
    "    center = centers[int(prediction)]\n",
    "    distance = np.linalg.norm(np.array(features) - np.array(center))\n",
    "    threshold = 1.0  # Define a threshold for anomaly detection\n",
    "    return \"anomaly\" if distance > threshold else \"normal\"\n",
    "\n",
    "# udf is user defined fucntions, applys any conditin on all the values in a col\n",
    "label_anomaly_udf = udf(label_anomaly, StringType())\n",
    "\n",
    "labeled_predictions = predictions.withColumn(\n",
    "    \"anomaly_label\", label_anomaly_udf(col(\"features\"), col(\"prediction\"))\n",
    ")\n",
    "\n",
    "print(\"Labeled Predictions:\")\n",
    "labeled_predictions.select(\"x\", \"y\", \"prediction\", \"anomaly_label\").show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09f654cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/lplab/.local/lib/python3.10/site-packages (2.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f6131cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial DataFrame:\n",
      "+----+----+-----+\n",
      "|   x|   y|label|\n",
      "+----+----+-----+\n",
      "| 0.0| 0.0|    0|\n",
      "| 0.1| 0.1|    0|\n",
      "| 0.2| 0.2|    0|\n",
      "| 9.0| 9.0|    1|\n",
      "| 9.1| 9.1|    1|\n",
      "| 9.2| 9.2|    1|\n",
      "|10.0|10.0|    1|\n",
      "+----+----+-----+\n",
      "\n",
      "+----+----+-----+-----------+-------+\n",
      "|   x|   y|label|   features|cluster|\n",
      "+----+----+-----+-----------+-------+\n",
      "| 0.0| 0.0|    0|  (2,[],[])|      1|\n",
      "| 0.1| 0.1|    0|  [0.1,0.1]|      1|\n",
      "| 0.2| 0.2|    0|  [0.2,0.2]|      1|\n",
      "| 9.0| 9.0|    1|  [9.0,9.0]|      0|\n",
      "| 9.1| 9.1|    1|  [9.1,9.1]|      0|\n",
      "| 9.2| 9.2|    1|  [9.2,9.2]|      0|\n",
      "|10.0|10.0|    1|[10.0,10.0]|      0|\n",
      "+----+----+-----+-----------+-------+\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'list' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17861/3616705441.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0manomaly_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manomaly_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_17861/3616705441.py\u001b[0m in \u001b[0;36mlabel_anomaly\u001b[0;34m(features, prediction)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m  \u001b[0;31m# Define a threshold for anomaly detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdistance\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m  \u001b[0;31m# 1 for anomaly, 0 for normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'list' and 'float'"
     ]
    }
   ],
   "source": [
    "# Implement code to evaluate the effectiveness of the K-means clustering model in detecting\n",
    "# anomalies.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KMeansAnomalyEvaluation\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (0.0, 0.0, 0),   # Normal\n",
    "    (0.1, 0.1, 0),   # Normal\n",
    "    (0.2, 0.2, 0),   # Normal\n",
    "    (9.0, 9.0, 1),   # Anomaly\n",
    "    (9.1, 9.1, 1),   # Anomaly\n",
    "    (9.2, 9.2, 1),   # Anomaly\n",
    "    (10.0, 10.0, 1)  # Anomaly\n",
    "]\n",
    "\n",
    "columns = [\"x\", \"y\", \"label\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Initial DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"x\", \"y\"], outputCol=\"features\")\n",
    "feature_df = assembler.transform(df)\n",
    "\n",
    "kmeans = KMeans(k=2, seed=1)\n",
    "model = kmeans.fit(feature_df)\n",
    "\n",
    "predictions = model.transform(feature_df)\n",
    "\n",
    "predictions = predictions.withColumnRenamed(\"prediction\", \"cluster\")\n",
    "predictions.show()\n",
    "\n",
    "# Step 5: Define anomaly detection function\n",
    "def label_anomaly(features, prediction):\n",
    "    center = model.clusterCenters()[int(prediction)]\n",
    "    distance = []\n",
    "    for i in range(len(features)):\n",
    "        distance.append(math.sqrt(features[i]**2+center[i]**2))\n",
    "    threshold = 2.0  # Define a threshold for anomaly detection\n",
    "    for i in range(len(features)):\n",
    "        if distance[i] > threshold:\n",
    "            return 1    # 1 for anomaly, 0 for normal\n",
    "\n",
    "def calculate_accuracy(true, predicted):\n",
    "    correct = sum(1 for true_label, pred_label in zip(true, predicted) if true_label == pred_label)\n",
    "    accuracy = correct / len(true)  # Divide by total number of predictions\n",
    "    return accuracy\n",
    "\n",
    "# Step 6: Collect results for evaluation\n",
    "results = []\n",
    "for row in predictions.collect():\n",
    "    features = row.features.toArray()\n",
    "    prediction = row.cluster\n",
    "    label = row.label\n",
    "    anomaly_label = label_anomaly(features, prediction)\n",
    "    results.append((label, anomaly_label))\n",
    "\n",
    "# Convert results to a DataFrame for evaluation\n",
    "results_df = spark.createDataFrame(results, [\"label\", \"predicted_anomaly\"])\n",
    "\n",
    "# Step 7: Evaluate effectiveness using accuracy\n",
    "result_pandas = results_df.toPandas()\n",
    "accuracy = calculate_accuracy(result_pandas['label'], result_pandas['predicted_anomaly'])\n",
    "\n",
    "# Show accuracy\n",
    "print(f\"Accuracy of the K-means anomaly detection model: {accuracy:.2f}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08ed8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
