{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e7ec77f-1a8b-4832-acd2-0b1ed1c4b292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pyspark 3.5.3\n",
      "Uninstalling pyspark-3.5.3:\n",
      "  Successfully uninstalled pyspark-3.5.3\n",
      "Collecting pyspark\n",
      "  Using cached pyspark-3.5.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall pyspark -y\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22b32c19-3cda-468b-89ee-dae2507eac45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o88.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (Chros executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 34 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 34 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 3. Show the original data\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Data:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 4. Data Transformation: Add a new column that categorizes people based on age\u001b[39;00m\n\u001b[0;32m     22\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAge_Group\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     23\u001b[0m                        when(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m30\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYoung\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39motherwise(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOld\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    962\u001b[0m     )\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o88.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (Chros executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 34 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 34 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# 1. Set up Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Simple PySpark Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Create sample data (you can replace this with reading a CSV file)\n",
    "data = spark.createDataFrame([\n",
    "    (1, \"Alice\", 23),\n",
    "    (2, \"Bob\", 34),\n",
    "    (3, \"Charlie\", 30),\n",
    "    (4, \"David\", 40)\n",
    "], [\"ID\", \"Name\", \"Age\"])\n",
    "\n",
    "# 3. Show the original data\n",
    "print(\"Original Data:\")\n",
    "data.show()\n",
    "\n",
    "# 4. Data Transformation: Add a new column that categorizes people based on age\n",
    "data = data.withColumn(\"Age_Group\", \n",
    "                       when(col(\"Age\") < 30, \"Young\").otherwise(\"Old\"))\n",
    "\n",
    "# 5. Show the transformed data\n",
    "print(\"Transformed Data:\")\n",
    "data.show()\n",
    "\n",
    "# 6. Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b466548d-de74-46b0-8a51-28ae3be6667f",
   "metadata": {},
   "source": [
    "## Prepreocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3639b555-7612-42ae-86e9-00248569839d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o163.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 3.0 failed 1 times, most recent failure: Lost task 4.0 in stage 3.0 (TID 18) (Chros executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 36 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 36 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[indexer, encoder, assembler, scaler, pca])\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# 7. Fit and transform the data\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m preprocessed_data \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(data)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Display the preprocessed data\u001b[39;00m\n\u001b[0;32m     48\u001b[0m preprocessed_data\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaled_features\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\ml\\pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o163.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 3.0 failed 1 times, most recent failure: Lost task 4.0 in stage 3.0 (TID 18) (Chros executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 36 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 36 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, PCA\n",
    "\n",
    "# 1. Set up Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data Preprocessing Sample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Create sample data\n",
    "data = spark.createDataFrame([\n",
    "    (1, 34.5, \"A\", 10.1),\n",
    "    (2, 36.1, \"B\", 15.3),\n",
    "    (3, 34.5, \"A\", 10.1),  # Duplicate row\n",
    "    (4, None, \"C\", 18.5),  # Missing value in feature1\n",
    "    (5, 29.2, None, 13.9),  # Missing value in categorical_column\n",
    "    (6, 32.5, \"A\", None),   # Missing value in feature3\n",
    "], [\"id\", \"feature1\", \"categorical_column\", \"feature3\"])\n",
    "\n",
    "# 3. Drop duplicates\n",
    "data = data.dropDuplicates()\n",
    "\n",
    "# 4. Handle missing values\n",
    "data = data.fillna({'feature1': 0, 'categorical_column': 'unknown', 'feature3': 0})\n",
    "\n",
    "# 5. Define preprocessing stages for the pipeline\n",
    "\n",
    "# a) Encode categorical variables\n",
    "indexer = StringIndexer(inputCol=\"categorical_column\", outputCol=\"categorical_index\")\n",
    "encoder = OneHotEncoder(inputCol=\"categorical_index\", outputCol=\"categorical_encoded\")\n",
    "\n",
    "# b) Assemble features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=[\"feature1\", \"feature3\", \"categorical_encoded\"], outputCol=\"features\")\n",
    "\n",
    "# c) Scale the feature vector\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "# d) (Optional) Apply PCA for dimensionality reduction\n",
    "pca = PCA(k=2, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
    "\n",
    "# 6. Build the pipeline\n",
    "pipeline = Pipeline(stages=[indexer, encoder, assembler, scaler, pca])\n",
    "\n",
    "# 7. Fit and transform the data\n",
    "preprocessed_data = pipeline.fit(data).transform(data)\n",
    "\n",
    "# Display the preprocessed data\n",
    "preprocessed_data.select(\"id\", \"features\", \"scaled_features\", \"pca_features\").show()\n",
    "\n",
    "# Print the schema of the processed data\n",
    "preprocessed_data.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52265693-a8ae-4a13-afdb-8a30e647c05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TO FILTER DATA\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Set up Spark session\n",
    "spark = SparkSession.builder.appName(\"Filter Year Column\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = spark.createDataFrame([\n",
    "    (1, 3),\n",
    "    (2, 6),\n",
    "    (3, 8),\n",
    "    (4, 2),\n",
    "    (5, 10)\n",
    "], [\"id\", \"year\"])\n",
    "\n",
    "print(\"Original Data:\")\n",
    "data.show()\n",
    "\n",
    "# Filter rows where the 'year' column is greater than 5\n",
    "filtered_data = data.filter(col(\"year\") > 5)\n",
    "\n",
    "print(\"Filtered Data (where year > 5):\")\n",
    "filtered_data.show()\n",
    "\n",
    "\n",
    "\n",
    "data = spark.createDataFrame([\n",
    "    (1, \"John Doe\"),\n",
    "    (2, \"Jane Smith\"),\n",
    "    (3, \"Johnny Depp\"),\n",
    "    (4, \"Alice Johnson\")\n",
    "], [\"id\", \"name\"])\n",
    "\n",
    "# Filter rows where name contains 'John'\n",
    "filtered_data = data.filter(col(\"name\").contains(\"John\"))\n",
    "filtered_data.show()\n",
    "\n",
    "\n",
    "\n",
    "# Sample data\n",
    "data = spark.createDataFrame([\n",
    "    (1, 25),\n",
    "    (2, None),\n",
    "    (3, 30),\n",
    "    (4, None)\n",
    "], [\"id\", \"age\"])\n",
    "\n",
    "# Filter rows where age is null\n",
    "filtered_data = data.filter(col(\"age\").isNull())\n",
    "filtered_data.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = spark.createDataFrame([\n",
    "    (1, 28, 45000),\n",
    "    (2, 22, 30000),\n",
    "    (3, 35, 60000),\n",
    "    (4, 26, 48000)\n",
    "], [\"id\", \"age\", \"salary\"])\n",
    "\n",
    "# Filter rows where age > 25 and salary < 50000\n",
    "filtered_data = data.filter((col(\"age\") > 25) & (col(\"salary\") < 50000))\n",
    "filtered_data.show()\n",
    "\n",
    "\n",
    "data = spark.createDataFrame([\n",
    "    (1, \"SuperPhone\"),\n",
    "    (2, \"UltraTablet Pro\"),\n",
    "    (3, \"Basic Watch\"),\n",
    "    (4, \"SuperLaptop Pro\")\n",
    "], [\"id\", \"product_name\"])\n",
    "\n",
    "# Filter rows where product_name starts with 'Super' or ends with 'Pro'\n",
    "filtered_data = data.filter((col(\"product_name\").startswith(\"Super\")) | (col(\"product_name\").endswith(\"Pro\")))\n",
    "filtered_data.show()\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import length\n",
    "\n",
    "data = spark.createDataFrame([\n",
    "    (1, \"Short text\"),\n",
    "    (2, \"This is a longer description\"),\n",
    "    (3, \"Medium size\"),\n",
    "    (4, \"Another example of long text\")\n",
    "], [\"id\", \"description\"])\n",
    "\n",
    "# Filter rows where description length is greater than 10\n",
    "filtered_data = data.filter(length(col(\"description\")) > 10)\n",
    "filtered_data.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "data = spark.createDataFrame([\n",
    "    (1, \"2023-03-15\"),\n",
    "    (2, \"2022-11-20\"),\n",
    "    (3, \"2023-07-10\"),\n",
    "    (4, \"2024-01-01\")\n",
    "], [\"id\", \"purchase_date\"])\n",
    "\n",
    "# Convert string to date type and filter rows in the range\n",
    "data = data.withColumn(\"purchase_date\", to_date(col(\"purchase_date\")))\n",
    "filtered_data = data.filter((col(\"purchase_date\") >= \"2023-01-01\") & (col(\"purchase_date\") <= \"2023-12-31\"))\n",
    "filtered_data.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e708280f-de50-4e6f-bd2b-a66a9f5689e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, current_date, datediff, col\n",
    "\n",
    "# Set up Spark session\n",
    "spark = SparkSession.builder.appName(\"Calculate Years Difference\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = spark.createDataFrame([\n",
    "    (1, \"2023-03-15\"),\n",
    "    (2, \"2021-07-10\"),\n",
    "    (3, \"2018-11-05\")\n",
    "], [\"id\", \"date_string\"])\n",
    "\n",
    "# Convert date_string to DateType\n",
    "data = data.withColumn(\"date\", to_date(\"date_string\", \"yyyy-MM-dd\"))\n",
    "\n",
    "# Calculate the difference in days, divide by 365, and add as a new column \"years\"\n",
    "data = data.withColumn(\"years\", datediff(current_date(), col(\"date\")) / 365)\n",
    "\n",
    "data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6da9496-7732-42b4-a29a-a4dec199c7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, sum\n",
    "\n",
    "# Set up Spark session\n",
    "spark = SparkSession.builder.appName(\"Agg Example 1\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = spark.createDataFrame([\n",
    "    (\"A\", 100),\n",
    "    (\"B\", 150),\n",
    "    (\"A\", 200),\n",
    "    (\"B\", 300),\n",
    "    (\"A\", 50)\n",
    "], [\"category\", \"sales\"])\n",
    "\n",
    "# Group by 'category' and calculate the average and sum of 'sales'\n",
    "result = data.groupBy(\"category\").agg(\n",
    "    avg(\"sales\").alias(\"avg_sales\"),\n",
    "    sum(\"sales\").alias(\"total_sales\")\n",
    ")\n",
    "\n",
    "result.show()\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Group by 'category' and count the number of rows in each group\n",
    "result = data.groupBy(\"category\").agg(\n",
    "    count(\"sales\").alias(\"count_sales\")\n",
    ")\n",
    "\n",
    "result.show()\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import max, min\n",
    "\n",
    "# Sample data with departments and salaries\n",
    "data = spark.createDataFrame([\n",
    "    (\"HR\", 50000),\n",
    "    (\"IT\", 75000),\n",
    "    (\"HR\", 60000),\n",
    "    (\"IT\", 80000),\n",
    "    (\"HR\", 55000),\n",
    "    (\"IT\", 70000)\n",
    "], [\"department\", \"salary\"])\n",
    "\n",
    "# Group by 'department' and calculate the maximum and minimum salary\n",
    "result = data.groupBy(\"department\").agg(\n",
    "    max(\"salary\").alias(\"max_salary\"),\n",
    "    min(\"salary\").alias(\"min_salary\")\n",
    ")\n",
    "\n",
    "result.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9d447a-5562-40a7-9462-4fd4cb7e1ae3",
   "metadata": {},
   "source": [
    "## Entity Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11020a2-dc5e-4ff1-9e57-7ac157165392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, lower, regexp_replace\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "import re\n",
    "\n",
    "spark = SparkSession.builder.appName('entity resolution').getOrCreate()\n",
    "df = spark.read.csv('customers-100.csv', header = True)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "# creating udfs\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    # note\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "    \n",
    "normal_udf = udf(normalize_text, StringType())\n",
    "\n",
    "df = df.withColumn('Index clean', normal_udf(col('Index')))\n",
    "df = df.withColumn('Customer Id clean', normal_udf(col('Customer Id')))\n",
    "df = df.withColumn('First Name clean', normal_udf(col('First Name')))\n",
    "df = df.withColumn('Last Name clean', normal_udf(col('Last Name')))\n",
    "df = df.withColumn('Company clean', normal_udf(col('Company')))\n",
    "df = df.withColumn('City clean', normal_udf(col('City')))\n",
    "df = df.withColumn('Country clean', normal_udf(col('Country')))\n",
    "df = df.withColumn('Phone 1 clean', normal_udf(col('Phone 1')))\n",
    "df = df.withColumn('Phone 2 clean', normal_udf(col('Phone 2')))\n",
    "df = df.withColumn('Email clean', normal_udf(col('Email')))\n",
    "df = df.withColumn('Subscription Date clean', normal_udf(col('Subscription Date')))\n",
    "df = df.withColumn('Website clean', normal_udf(col('Website')))\n",
    "\n",
    "\n",
    "'''\n",
    "tokenizer = Tokenizer(inputCol=\"text_column\", outputCol=\"text_column_tokenized\")\n",
    "data = tokenizer.transform(data)\n",
    "\n",
    "# Step 2: Replace the original column with the transformed column\n",
    "# Drop the old column and rename the new column\n",
    "data = data.drop(\"text_column\").withColumnRenamed(\"text_column_tokenized\", \"text_column\")\n",
    "\n",
    "print(\"Data After Replacing Column:\")\n",
    "data.show()\n",
    "\n",
    "'''\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(inputCol= 'First Name clean', outputCol='tokens')\n",
    "df_tokenized = tokenizer.transform(df)\n",
    "\n",
    "# remove stop words\n",
    "stop_rem = StopWordsRemover(inputCol= 'tokens', outputCol='filtered_tokens')\n",
    "df_filtered = stop_rem.transform(df_tokenized)\n",
    "\n",
    "df_filtered.show(5)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f003fef-a88d-4609-990c-bbd60936511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, udf\n",
    "from pyspark.sql.types import FloatType, ArrayType, StringType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "import re\n",
    "\n",
    "spark = SparkSession.builder.appName('entity resolution').getOrCreate()\n",
    "df = spark.read.csv('customers-100.csv', header = True)\n",
    "\n",
    "def metric(x, y):\n",
    "    x, y = set(x), set(y)\n",
    "    u = x.union(y)\n",
    "    i = x.intersection(y)\n",
    "    if len(i) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return len(i) / len(u)\n",
    "metrix_udf = udf(metric, FloatType())\n",
    "\n",
    "def list_convert(t):\n",
    "    return list(t)\n",
    "convert = udf(list_convert, ArrayType(StringType()))\n",
    "\n",
    "df = df.withColumn(\"Phone 1 Set\", convert(col(\"Phone 1\")))\n",
    "df = df.withColumn(\"Phone 2 Set\", convert(col(\"Phone 2\")))\n",
    "df = df.withColumn(\"metric\", metrix_udf(col(\"Phone 1 Set\"), col(\"Phone 2 Set\")))\n",
    "\n",
    "df.select('Phone 1', 'Phone 2', 'metric').show()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8798e96a-d4e1-4ed4-8857-0da1c354c9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EntityResolutionEvaluation\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data: (id1, id2, true_label, predicted_label)\n",
    "data = [(1, 2, 1, 1),  # 1: True Positive\n",
    "        (1, 3, 1, 0),  # 2: False Negative\n",
    "        (2, 3, 0, 1),  # 3: False Positive\n",
    "        (2, 4, 0, 0)]  # 4: True Negative\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id1\", \"id2\", \"true_label\", \"predicted_label\"])\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "# spark needs proper brackets for boolean and & like C\n",
    "confusion_matrix = df.groupBy().agg(\n",
    "    count(when((col(\"true_label\") == 1) & (col(\"predicted_label\") == 1), 1)).alias(\"TP\"),  # True Positives\n",
    "    count(when((col(\"true_label\") == 1) & (col(\"predicted_label\") == 0), 1)).alias(\"FN\"),  # False Negatives\n",
    "    count(when((col(\"true_label\") == 0) & (col(\"predicted_label\") == 1), 1)).alias(\"FP\"),  # False Positives\n",
    "    count(when((col(\"true_label\") == 0) & (col(\"predicted_label\") == 0), 1)).alias(\"TN\")   # True Negatives\n",
    ")\n",
    "\n",
    "# Calculate Precision, Recall, F1-Score\n",
    "metrics = confusion_matrix.select(\n",
    "    (col(\"TP\") / (col(\"TP\") + col(\"FP\"))).alias(\"Precision\"),\n",
    "    (col(\"TP\") / (col(\"TP\") + col(\"FN\"))).alias(\"Recall\"),\n",
    "    (2 * col(\"TP\") / (2 * col(\"TP\") + col(\"FP\") + col(\"FN\"))).alias(\"F1_Score\")\n",
    ")\n",
    "\n",
    "# Show metrics\n",
    "metrics.show(truncate=False)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86159719-ecdb-4bec-85ca-cd8b2805650b",
   "metadata": {},
   "source": [
    "## Recommender Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb8d8fa-4c39-4238-b1c3-21a5054a7de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "import json\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster('local[*]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaa1783-32b3-4bcf-b8cf-6ebaa340f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['product_id', 'user_id', 'score', 'time']\n",
    "\n",
    "def validate(line):\n",
    "    for x in fields:\n",
    "        if x not in line:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "rdd = sc.textFile('movies.json')\n",
    "review = rdd.map(lambda x: json.loads(x)).filter(validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d120d1-f8e6-4951-92ab-970ac4377640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no. of movies\n",
    "r1 = review.groupBy(lambda x: x['product_id']).count()\n",
    "print(r1)\n",
    "# no. of users\n",
    "r2 = review.groupBy(lambda x: x['user_id']).count()\n",
    "print(r2)\n",
    "# no. of reviews\n",
    "print(review.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a518dce2-4c82-4398-935a-ec77c438c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no. of people watching each product\n",
    "r1 = review.map(lambda x: (x['product_id'], 1)).reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1], ascending = False)\n",
    "r1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6441d37-33c4-4c77-832e-29fa7f46d207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# has a particular person written a review\n",
    "george = review.filter(lambda x: 'George' in x['profile_name'])\n",
    "print(f'George has written {george.count()} no. of reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7b180f-2268-4372-a669-191cc602b719",
   "metadata": {},
   "outputs": [],
   "source": [
    "george.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec714f-e730-47c0-983c-0c08d992fde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best rated movie\n",
    "r1 = review.map(lambda x: (x['product_id'], x['score'])).reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1], ascending= False)\n",
    "r1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e9e016-f125-42a5-9543-61113577dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# worst rated movie\n",
    "r2 = review.map(lambda x: (x['product_id'], x['score'])).reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1], ascending= True)\n",
    "r2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2977f6-50cf-472f-8a4c-86b6c2a766d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "import hashlib\n",
    "\n",
    "def get_hash(x):\n",
    "    return int(hashlib.sha1(x).hexdigest(), 16) % (10 ** 8)\n",
    "\n",
    "rating = review.map(lambda x: (get_hash(x['user_id'].encode('utf-8')), get_hash(x['product_id'].encode('utf-8')), int(x['score'])))\n",
    "train = rating.filter(lambda x: ((x[0] + x[1]) % 10) >= 2)\n",
    "test = rating.filter(lambda x: ((x[0] + x[1]) % 10) < 2)\n",
    "# no. of training samples\n",
    "print(train.count())\n",
    "# no. of testing samples\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1acfe5c-0f27-4259-8caa-26f9e8ca8625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model\n",
    "rank = 10\n",
    "numIterations = 10  # Corrected parameter name\n",
    "model = ALS.train(train, rank, numIterations)\n",
    "\n",
    "# Prepare test data for prediction\n",
    "format_test = test.map(lambda x: (int(x[0]), int(x[1])))\n",
    "predictions = model.predictAll(format_test).map(lambda x: ((x[0], x[1]), x[2]))\n",
    "\n",
    "# Joining actual and predicted ratings\n",
    "true_and_predicted = test.map(lambda x: ((int(x[0]), int(x[1])), x[2])).join(predictions)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = true_and_predicted.map(lambda x: (x[1][0] - x[1][1]) ** 2).mean()\n",
    "print(f\"Mean Squared Error = {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3521b266-fc85-45fb-b809-7f2b9dd4c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_reviews = review.filter(lambda x: x['score'] == 5.0)\n",
    "bad_reviews = review.filter(lambda x: x['score'] == 1.0)\n",
    "\n",
    "good_words = good_reviews.flatMap(lambda x: x['review'].split())\n",
    "good_words = good_words.map(lambda x: (x.strip(), 1)).reduceByKey(lambda x, y: x + y).filter(lambda x: x[1] > 20)\n",
    "no_good = good_words.count()\n",
    "\\\n",
    "\n",
    "bad_words = bad_reviews.flatMap(lambda x: x['review'].split())\n",
    "bad_words = bad_words.map(lambda x: (x.strip(), 1)).reduceByKey(lambda x, y: x + y).filter(lambda x: x[1] > 20)\n",
    "no_bad = bad_words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d875cc-5572-4730-a161-97a3834fe002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import fabs\n",
    "# frequencies of words\n",
    "frequency_good = good_words.map(lambda x: (x[0], float(x[1]) / no_good))\n",
    "frequency_bad = bad_words.map(lambda x: (x[0], float(x[1]) / no_bad))\n",
    "\n",
    "# joined frequencies\n",
    "joined = frequency_good.join(frequency_bad)\n",
    "\n",
    "# relative difference between good and bad score\n",
    "result = joined.map(lambda x: (x[0], fabs(x[1][0] - x[1][1]) / x[1][0])).sortBy(lambda x: x[1], ascending = False)\n",
    "result.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7914a3-5ac6-4aa8-a886-712184e2c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "plt.title('histogram')\n",
    "plt.xlabel('word')\n",
    "plt.ylabel('no. of occurances')\n",
    "for x in result.take(7):\n",
    "    plt.bar(x[0], x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b70508d-fdd6-463e-a67a-09550ab25bf3",
   "metadata": {},
   "source": [
    "## Descision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738923cc-7716-49ef-a300-fe24ff0fecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkSession.builder.appName('decision trees').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fa9b2a-36f2-4f08-8bd5-9fcc41f7d38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_header = sc.read.option('inferSchema', True).option('header', False).csv('covtype.csv')\n",
    "data_without_header.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774377f4-f944-424b-aa99-e1327edf0335",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DTTestWithTuning\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = spark.read.option(\"inferSchema\", True).option(\"header\", False).csv(\"covtype.csv\")\n",
    "\n",
    "# Define column names\n",
    "colnames = [\"Elevation\", \"Aspect\", \"Slope\", \"Horizontal_Distance_To_Hydrology\", \n",
    "            \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\", \n",
    "            \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\", \n",
    "            \"Horizontal_Distance_To_Fire_Points\"]\n",
    "colnames = colnames + [f'Wilderness_Area_{i}' for i in range(4)]\n",
    "colnames = colnames + [f'Soil_Type{i}' for i in range(40)] + ['Cover_Type']\n",
    "\n",
    "# Convert the data to a DataFrame\n",
    "data = data.toDF(*colnames)\n",
    "data = data.withColumn('Cover_Type', col('Cover_Type').cast(DoubleType()))\n",
    "\n",
    "# Input columns (features) excluding the label column\n",
    "inp_col = colnames[:len(colnames) - 1]\n",
    "\n",
    "# Use VectorAssembler to combine features into a single feature vector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vec_ass = VectorAssembler(inputCols=inp_col, outputCol='feature_vec')\n",
    "vec_data = vec_ass.transform(data)\n",
    "\n",
    "# Import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Initialize the classifier\n",
    "classifier = DecisionTreeClassifier(labelCol='Cover_Type', featuresCol='feature_vec', predictionCol='prediction')\n",
    "\n",
    "# Set up a parameter grid for tuning hyperparameters\n",
    "param_grid = (ParamGridBuilder()\n",
    "              .addGrid(classifier.maxDepth, [5, 10, 15])\n",
    "              .addGrid(classifier.maxBins, [32, 64])\n",
    "              .addGrid(classifier.impurity, ['gini', 'entropy'])\n",
    "              .build())\n",
    "\n",
    "# Define the evaluator to evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='Cover_Type', predictionCol='prediction', metricName='accuracy')\n",
    "\n",
    "# Set up cross-validation with 3 folds\n",
    "crossval = CrossValidator(estimator=classifier, \n",
    "                          estimatorParamMaps=param_grid, \n",
    "                          evaluator=evaluator, \n",
    "                          numFolds=3)\n",
    "\n",
    "# Train the model using cross-validation\n",
    "cv_model = crossval.fit(vec_data)\n",
    "\n",
    "# Best model and its parameters\n",
    "best_model = cv_model.bestModel\n",
    "print(f\"Best Model Parameters: Depth: {best_model._java_obj.getMaxDepth()}, MaxBins: {best_model._java_obj.getMaxBins()}, Impurity: {best_model._java_obj.getImpurity()}\")\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy = evaluator.evaluate(best_model.transform(vec_data))\n",
    "print(f\"Best Model Accuracy: {accuracy}\")\n",
    "\n",
    "# Get the feature importances from the best model\n",
    "df = pd.DataFrame(best_model.featureImportances.toArray(), index=inp_col, columns=[\"importance\"]).sort_values(by=\"importance\", ascending=False)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ab587-3933-42e9-9ab9-25be88435451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DTTestUsingRDD\").getOrCreate()\n",
    "\n",
    "# Read the data into an RDD\n",
    "rdd_data = spark.sparkContext.textFile(\"covtype.csv\")\n",
    "\n",
    "# Define column names\n",
    "colnames = [\"Elevation\", \"Aspect\", \"Slope\", \"Horizontal_Distance_To_Hydrology\", \n",
    "            \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\", \n",
    "            \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\", \n",
    "            \"Horizontal_Distance_To_Fire_Points\"] + \\\n",
    "           [f'Wilderness_Area_{i}' for i in range(4)] + \\\n",
    "           [f'Soil_Type{i}' for i in range(40)] + ['Cover_Type']\n",
    "\n",
    "# Clean data: removing the header and handling column parsing manually\n",
    "header = rdd_data.first()\n",
    "rdd_data = rdd_data.filter(lambda x: x != header)\n",
    "\n",
    "# Convert the RDD to a structured format, splitting columns and converting to float\n",
    "parsed_rdd = rdd_data.map(lambda line: line.split(\",\"))\n",
    "parsed_rdd = parsed_rdd.map(lambda cols: [float(x) for x in cols])  # Simplified conversion\n",
    "\n",
    "# Create feature vectors and labels\n",
    "features_rdd = parsed_rdd.map(lambda x: Vectors.dense(x[:-1]))\n",
    "labels_rdd = parsed_rdd.map(lambda x: x[-1])\n",
    "\n",
    "# Check unique labels and adjust them if necessary\n",
    "unique_labels = labels_rdd.distinct().collect()\n",
    "print(\"Unique labels before indexing:\", unique_labels)\n",
    "\n",
    "# Create a mapping for labels if they are not starting from 0\n",
    "label_mapping = {label: idx for idx, label in enumerate(sorted(unique_labels))}\n",
    "print(\"Label mapping:\", label_mapping)\n",
    "\n",
    "# Map original labels to new indices\n",
    "indexed_labels_rdd = labels_rdd.map(lambda x: label_mapping[x])\n",
    "\n",
    "# Combine features and indexed labels into LabeledPoint for training\n",
    "labeled_rdd = features_rdd.zip(indexed_labels_rdd).map(lambda x: LabeledPoint(float(x[1]), x[0]))\n",
    "\n",
    "# Debugging: Check number of labeled points\n",
    "print(\"Number of labeled points:\", labeled_rdd.count())\n",
    "\n",
    "# Debugging: Check first few LabeledPoints\n",
    "print(\"First 5 LabeledPoints:\", labeled_rdd.take(5))\n",
    "\n",
    "# Train a DecisionTree model using MLlib\n",
    "try:\n",
    "    num_classes = len(label_mapping)  # Adjust numClasses based on unique labels\n",
    "    model = DecisionTree.trainClassifier(labeled_rdd, numClasses=num_classes, \n",
    "                                         categoricalFeaturesInfo={}, \n",
    "                                         impurity='gini', maxDepth=5, maxBins=32)\n",
    "    # Print model summary (equivalent to .toDebugString)\n",
    "    print(model.toDebugString())\n",
    "    \n",
    "    # Get feature importances from the model and sort them\n",
    "    feature_importances = model.featureImportances.toArray()\n",
    "    feature_importance_df = pd.DataFrame(feature_importances, index=colnames[:-1], columns=[\"importance\"]) \\\n",
    "        .sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "    # Show the feature importance\n",
    "    print(feature_importance_df)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error during model training:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5188f8-238b-45c9-b1c5-04edb2bdb390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "colnames =  [\"Elevation\", \"Aspect\", \"Slope\", \"Horizontal_Distance_To_Hydrology\", \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\", \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\", \"Horizontal_Distance_To_Fire_Points\"]\n",
    "colnames = colnames + [f'Wilderness_Area_{i}' for i in range(4)]\n",
    "colnames = colnames + [f'Soil_Type{i}' for i in range(40)] + ['Cover_Type']\n",
    "\n",
    "data = data_without_header.toDF(*colnames)\n",
    "data = data.withColumn('Cover_Type', col('Cover_Type').cast(DoubleType()))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdad6a79-b8f7-408f-a092-61a80735ffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4b0aed-93a4-464b-b99d-9335e00594fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "input_cols = colnames[: len(colnames) - 1]\n",
    "print(input_cols)\n",
    "vector_assembler = VectorAssembler(inputCols= input_cols, outputCol = 'featureVector')\n",
    "assembled_train = vector_assembler.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795b4871-8c67-409f-90ca-df8aa6b7ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier(seed = 1234, labelCol= 'Cover_Type', featuresCol= 'featureVector', predictionCol= 'prediction')\n",
    "model = classifier.fit(assembled_train)\n",
    "print(model.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a1ffa7-f815-4faf-aaca-dd71bd0bd497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(model.featureImportances.toArray(), index= input_cols, columns= ['importance']).sort_values(by= 'importance', ascending= False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a6f2b5-1138-41b3-80c8-647595bde04d",
   "metadata": {},
   "source": [
    "## Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2ea39b-7906-4236-be6e-a3c377de8335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sp = SparkSession.builder.appName('a').getOrCreate()\n",
    "data = sp.read.option('inferSchema', True).option('header', False).csv('corrected.csv')\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d98fc6-6530-440b-b543-4146c26edfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = [ \"duration\", \"protocol_type\", \"service\", \"flag\",\n",
    " \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n",
    " \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n",
    " \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
    " \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
    " \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n",
    " \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
    " \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n",
    " \"dst_host_count\", \"dst_host_srv_count\",\n",
    " \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
    " \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n",
    " \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    " \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n",
    " \"label\"]\n",
    "data = data.toDF(*colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2368ea4b-4901-4c9d-bfaf-7d148e68a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473cf921-04e8-49e4-bb34-d8e0fa5d5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "def oneHotPipeline(inputcol):\n",
    "    indexer = StringIndexer(inputCol = inputcol, outputCol = inputcol + 'index')\n",
    "    encoder = OneHotEncoder(inputCol = inputcol + 'index', outputCol = inputcol + '_vec')\n",
    "    pipe = Pipeline(stages = [indexer, encoder])\n",
    "    return pipe, inputcol + '_vec'\n",
    "\n",
    "def clustering_score(data, k):\n",
    "    proto_pipe, proto = oneHotPipeline('protocol_type')\n",
    "    serv_pipe, serv = oneHotPipeline('service')\n",
    "    flag_pipe, flag_vec = oneHotPipeline('flag')\n",
    "\n",
    "    cols = set(data.columns) - set(['label', 'protocol_type', 'service', 'flag'])\n",
    "    cols.add(proto)\n",
    "    cols.add(serv)\n",
    "    cols.add(flag_vec)\n",
    "    cols = list(cols)\n",
    "    assembler = VectorAssembler(inputCols = cols, outputCol = 'features')\n",
    "    scaler = StandardScaler(inputCol = 'features', outputCol = 'scaledfeatures', withStd = True, withMean = False)\n",
    "    kmeans = KMeans(seed = 1234, k = k, maxIter = 20, tol = 1.0e-5, predictionCol = 'cluster', featuresCol = 'scaledfeatures')\n",
    "    pipe = Pipeline(stages = [proto_pipe, serv_pipe, flag_pipe, assembler, scaler, kmeans])\n",
    "    model = pipe.fit(data)\n",
    "    kmodel = model.stages[-1]\n",
    "    return kmodel.summary.trainingCost\n",
    "\n",
    "for k in range(60, 271, 30):\n",
    "    print(k, clustering_score(data, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d105b2-5fe1-461e-80f3-c471f6f6e6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"KMeansExample\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame (replace with your actual data)\n",
    "data = spark.createDataFrame([\n",
    "    (0, 1.0, 1.1),\n",
    "    (1, 1.2, 1.3),\n",
    "    (2, 1.4, 1.5),\n",
    "    (3, 10.0, 10.1),\n",
    "    (4, 10.2, 10.3),\n",
    "    (5, 10.4, 10.5)\n",
    "], [\"id\", \"feature1\", \"feature2\"])\n",
    "\n",
    "# Assemble features into a vector column\n",
    "assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\"], outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "\n",
    "# KMeans Clustering\n",
    "kmeans = KMeans().setK(2).setSeed(1).setFeaturesCol(\"features\").setPredictionCol(\"prediction\")\n",
    "\n",
    "# Fit the model\n",
    "model = kmeans.fit(data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(data)\n",
    "\n",
    "# Show the results\n",
    "predictions.show()\n",
    "\n",
    "# Get the clustering centers (centroids)\n",
    "centroids = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centroids:\n",
    "    print(center)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f559ed-b564-4db1-94bd-07e9d50fbebd",
   "metadata": {},
   "source": [
    "## Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe64d13-471e-47f8-9802-0fc152aa874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, pow, count, lit\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"MonteCarloPiEstimation\").getOrCreate()\n",
    "\n",
    "# Define the list of CSV files\n",
    "csv_files = [\"points1.csv\", \"points2.csv\", \"points3.csv\"]\n",
    "\n",
    "# Function to calculate points within the unit circle\n",
    "def calculate_points_in_circle(df):\n",
    "    return df.withColumn(\"in_circle\", (pow(col(\"x\"), 2) + pow(col(\"y\"), 2) <= 1).cast(\"int\"))\n",
    "\n",
    "# Initialize counters for total points and points within the circle\n",
    "total_points = 0\n",
    "points_in_circle = 0\n",
    "\n",
    "# Process each CSV file\n",
    "for file in csv_files:\n",
    "    # Load data\n",
    "    df = spark.read.csv(file, header=True, inferSchema=True)\n",
    "    \n",
    "    # Calculate points inside the circle\n",
    "    df = calculate_points_in_circle(df)\n",
    "    \n",
    "    # Aggregate counts\n",
    "    total_points += df.count()\n",
    "    points_in_circle += df.filter(col(\"in_circle\") == 1).count()\n",
    "\n",
    "# Estimate Pi\n",
    "pi_estimate = 4 * (points_in_circle / total_points)\n",
    "print(f\"Estimated value of Pi: {pi_estimate}\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c0348a-27d0-42ee-91c5-1df5583af584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lag, log\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"MonteCarloStockSimulation\").getOrCreate()\n",
    "\n",
    "# Load historical stock data\n",
    "df = spark.read.csv(\"stock_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Calculate daily returns (log returns)\n",
    "window = Window.orderBy(\"date\")\n",
    "df = df.withColumn(\"prev_price\", lag(\"price\").over(window))\n",
    "df = df.withColumn(\"log_return\", log(col(\"price\") / col(\"prev_price\")))\n",
    "\n",
    "# Drop rows with null values (first row of log_return is null)\n",
    "df = df.na.drop(subset=[\"log_return\"])\n",
    "\n",
    "# Calculate mean and standard deviation of log returns\n",
    "log_return_stats = df.select(\"log_return\")\n",
    "mean_return = log_return_stats.groupBy().mean().first()[0]\n",
    "std_dev_return = log_return_stats.groupBy().agg({\"log_return\": \"stddev\"}).first()[0]\n",
    "\n",
    "# Parameters for simulation\n",
    "start_price = df.select(\"price\").orderBy(\"date\", ascending=False).first()[0]  # Latest stock price\n",
    "num_simulations = 1000  # Number of Monte Carlo paths\n",
    "num_days = 252  # Number of days (e.g., 1 year of trading days)\n",
    "\n",
    "# Monte Carlo simulation function\n",
    "def monte_carlo_simulation():\n",
    "    prices = [start_price]\n",
    "    for _ in range(num_days):\n",
    "        # Simulate daily return using random normal with mean and standard deviation\n",
    "        daily_return = np.random.normal(mean_return, std_dev_return)\n",
    "        # Calculate next price using numpy exp function\n",
    "        next_price = prices[-1] * np.exp(daily_return)\n",
    "        prices.append(next_price)\n",
    "    return prices\n",
    "\n",
    "# Run simulations\n",
    "results = [monte_carlo_simulation() for _ in range(num_simulations)]\n",
    "\n",
    "# Define schema for DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"simulation\", IntegerType(), True),\n",
    "    StructField(\"day\", IntegerType(), True),\n",
    "    StructField(\"price\", FloatType(), True)  # Use FloatType to avoid issues with float64\n",
    "])\n",
    "\n",
    "# Convert results to a DataFrame with explicit schema\n",
    "simulated_data = spark.createDataFrame(\n",
    "    [(i, day, float(price)) for i, simulation in enumerate(results) for day, price in enumerate(simulation)],\n",
    "    schema=schema\n",
    ")\n",
    "\n",
    "# Calculate average ending price after all simulations\n",
    "avg_ending_price = simulated_data.filter(col(\"day\") == num_days).groupBy().avg(\"price\").first()[0]\n",
    "print(f\"Estimated average ending price after {num_days} days: {avg_ending_price}\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac471fc-feb5-4976-8370-7b2f7f207ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"MergeFilesFromArray\").getOrCreate()\n",
    "\n",
    "# Example list of file paths (CSV files in this case)\n",
    "file_paths = [\"path/to/file1.csv\", \"path/to/file2.csv\", \"path/to/file3.csv\"]\n",
    "\n",
    "# Read and merge all the files into a single DataFrame\n",
    "df_list = [spark.read.option(\"header\", \"true\").csv(file) for file in file_paths]\n",
    "\n",
    "# Union all DataFrames in the list\n",
    "merged_data = df_list[0]\n",
    "for df in df_list[1:]:\n",
    "    merged_data = merged_data.union(df)\n",
    "\n",
    "# Show the result\n",
    "merged_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f43f19-c239-4e36-a3d0-43e6f91b8ef0",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a1296-58bf-4d51-a21b-9f67a304dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"LinearRegressionExample\").getOrCreate()\n",
    "\n",
    "# Sample data (replace with your own data or load from a CSV)\n",
    "data = [\n",
    "    (1, 1.0, 3.0),\n",
    "    (2, 2.0, 6.0),\n",
    "    (3, 3.0, 9.0),\n",
    "    (4, 4.0, 12.0),\n",
    "    (5, 5.0, 15.0)\n",
    "]\n",
    "\n",
    "# Define the schema\n",
    "columns = [\"id\", \"feature\", \"label\"]\n",
    "\n",
    "# Create DataFrame from sample data\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Assemble features into a vector (necessary for ML algorithms in PySpark)\n",
    "assembler = VectorAssembler(inputCols=[\"feature\"], outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Train the model\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Print the coefficients and intercept\n",
    "print(f\"Coefficients: {lr_model.coefficients}\")\n",
    "print(f\"Intercept: {lr_model.intercept}\")\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Show predictions\n",
    "predictions.select(\"id\", \"label\", \"prediction\").show()\n",
    "\n",
    "# Evaluate the model using RMSE and R2\n",
    "print(\"Root Mean Squared Error (RMSE):\", lr_model.summary.rootMeanSquaredError)\n",
    "print(\"R2:\", lr_model.summary.r2)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e7f1fe-f612-4656-906a-c7bd2a397bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
