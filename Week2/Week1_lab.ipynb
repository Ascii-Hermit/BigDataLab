{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f1a0f69-7412-4564-ad6a-3eab3297f6c3",
   "metadata": {},
   "source": [
    "# Apache Spark is written in Scala programming language. \n",
    "# To support Python with Spark, Apache Spark Community released a tool, PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eed5c46e-c466-4db5-a218-49161a0a677b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name| Id|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "|  Bob|  2|\n",
      "|Cathy|  3|\n",
      "+-----+---+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Id: long (nullable = true)\n",
      "\n",
      "+-----+\n",
      "| Name|\n",
      "+-----+\n",
      "|Alice|\n",
      "|  Bob|\n",
      "|Cathy|\n",
      "+-----+\n",
      "\n",
      "+-----+---+\n",
      "| Name| Id|\n",
      "+-----+---+\n",
      "|  Bob|  2|\n",
      "|Cathy|  3|\n",
      "+-----+---+\n",
      "\n",
      "+-----+---+---------+\n",
      "| Name| Id|IdPlusOne|\n",
      "+-----+---+---------+\n",
      "|Alice|  1|        2|\n",
      "|  Bob|  2|        3|\n",
      "|Cathy|  3|        4|\n",
      "+-----+---+---------+\n",
      "\n",
      "+-----+---+---------+\n",
      "| Name| Id|IdPlusOne|\n",
      "+-----+---+---------+\n",
      "|  Bob|  2|        3|\n",
      "|Cathy|  3|        4|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## BASIC PYSPARK PROGRAM\n",
    "\n",
    "# pip install pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a DataFrame from a list of tuples\n",
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Cathy\", 3)]\n",
    "columns = [\"Name\", \"Id\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Show the schema\n",
    "df.printSchema()\n",
    "\n",
    "# Select a single column\n",
    "df.select(\"Name\").show()\n",
    "\n",
    "# imposing conditions\n",
    "df.filter(df.Id > 1).show()\n",
    "\n",
    "# for adding columns\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"IdPlusOne\", col(\"Id\") + 1)\n",
    "df.show()\n",
    "\n",
    "# running SQL queries\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "result = spark.sql(\"SELECT * FROM people WHERE Id > 1\")\n",
    "result.show()\n",
    "\n",
    "# reading and writng to csv file\n",
    "# Read CSV file\n",
    "df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
    "df.write.csv(\"path/to/output.csv\", header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553c243-c7aa-439f-bb1e-4f2cf7b6882f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----+\n",
      "| Name|Category|Value|\n",
      "+-----+--------+-----+\n",
      "|Alice|       A|    1|\n",
      "|  Bob|       B|    2|\n",
      "|Alice|       A|    3|\n",
      "|  Bob|       B|    4|\n",
      "|Alice|       C|    5|\n",
      "+-----+--------+-----+\n",
      "\n",
      "+-----+-----+---+-------+---+---+\n",
      "| Name|Count|Sum|Average|Min|Max|\n",
      "+-----+-----+---+-------+---+---+\n",
      "|Alice|    3|  9|    3.0|  1|  5|\n",
      "|  Bob|    2|  6|    3.0|  2|  4|\n",
      "+-----+-----+---+-------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Working with aggregations\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, avg, min, max, count\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AggregationExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [(\"Alice\", \"A\", 1), (\"Bob\", \"B\", 2), (\"Alice\", \"A\", 3), (\"Bob\", \"B\", 4), (\"Alice\", \"C\", 5)]\n",
    "columns = [\"Name\", \"Category\", \"Value\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "# Aggregations\n",
    "result = df.groupBy(\"Name\").agg(\n",
    "    count(\"Value\").alias(\"Count\"),\n",
    "    sum(\"Value\").alias(\"Sum\"),\n",
    "    avg(\"Value\").alias(\"Average\"),\n",
    "    min(\"Value\").alias(\"Min\"),\n",
    "    max(\"Value\").alias(\"Max\")\n",
    ")\n",
    "\n",
    "result.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d58320-add8-4633-a6a7-ca41a1f2d517",
   "metadata": {},
   "source": [
    "## LAB EXERCISES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c496f1aa-09e3-437c-b813-d91480c6251a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/03 15:00:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "+------+\n",
      "\n",
      "Squared DataFrame:\n",
      "+------+-------+\n",
      "|number|squared|\n",
      "+------+-------+\n",
      "|     1|    1.0|\n",
      "|     2|    4.0|\n",
      "|     3|    9.0|\n",
      "|     4|   16.0|\n",
      "|     5|   25.0|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "# Write a PySpark program to square set of integers.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SquareIntegers\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# NOTE THE COMMAS ARE NECESSARY AS THE INPUT MUST BE A TUPLE NOT INT\n",
    "data = [(1,),(2,),(3,),(4,),(5,)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"number\"])\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# Adding another column to of squared numbers\n",
    "df = df.withColumn(\"squared\", col(\"number\") ** 2)\n",
    "\n",
    "print(\"Squared DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4802c8c0-e2a1-4978-91df-5a46376eacfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     1|\n",
      "|     5|\n",
      "|     3|\n",
      "|     9|\n",
      "|     7|\n",
      "+------+\n",
      "\n",
      "Maximum value: 9\n"
     ]
    }
   ],
   "source": [
    "# Question 2 Write a PySpark program to find the maximum of given set of numbers.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Find Maximum\") \\\n",
    "    .getOrCreate()\n",
    "data = [(1,), (5,), (3,), (9,), (7,)]\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "df = spark.createDataFrame(data, [\"number\"])\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# WE ADD [0][0] AT THE END TO GET HTE EXACT VALUE\n",
    "max_value = df.agg(max(\"number\")).collect()[0][0]\n",
    "print(f\"Maximum value: {max_value}\")\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "\n",
    "# WE ADD [0][0] AT THE END TO GET HTE EXACT VALUE\n",
    "max_value = df.agg(max(\"number\")).collect()[0][0]\n",
    "print(f\"Maximum value: {max_value}\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1ca790c-cb07-4a17-b0d9-0bfb259b7ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     1|\n",
      "|     5|\n",
      "|     3|\n",
      "|     9|\n",
      "|     7|\n",
      "+------+\n",
      "\n",
      "Maximum value: 5.0\n"
     ]
    }
   ],
   "source": [
    "#Question 3 Write a PySpark program to find average of N numbers\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Find Maximum\") \\\n",
    "    .getOrCreate()\n",
    "data = [(1,), (5,), (3,), (9,), (7,)]\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "df = spark.createDataFrame(data, [\"number\"])\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# WE ADD [0][0] AT THE END TO GET HTE EXACT VALUE\n",
    "max_value = df.agg(avg(\"number\")).collect()[0][0]\n",
    "print(f\"Maximum value: {max_value}\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96cb9fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            Industry|\n",
      "+--------------------+\n",
      "|  Accounting/Finance|\n",
      "|Advertising/Publi...|\n",
      "|  Aerospace/Aviation|\n",
      "|Arts/Entertainmen...|\n",
      "|          Automotive|\n",
      "|    Banking/Mortgage|\n",
      "|Business Development|\n",
      "|Business Opportunity|\n",
      "|Clerical/Administ...|\n",
      "|Construction/Faci...|\n",
      "|      Consumer Goods|\n",
      "|    Customer Service|\n",
      "|  Education/Training|\n",
      "|    Energy/Utilities|\n",
      "|         Engineering|\n",
      "| Government/Military|\n",
      "|               Green|\n",
      "|          Healthcare|\n",
      "|  Hospitality/Travel|\n",
      "|     Human Resources|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 4 Demonstrate how to read a CSV file into a PySpark DataFrame.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"industry.csv\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9097b41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Industry='Accounting/Finance'),\n",
       " Row(Industry='Advertising/Public Relations'),\n",
       " Row(Industry='Aerospace/Aviation'),\n",
       " Row(Industry='Arts/Entertainment/Publishing'),\n",
       " Row(Industry='Automotive')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 5 Use PySpark commands to display the first few rows and schema of a DataFrame.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read top 5 rows CSV\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.load(\"industry.csv\", format = \"csv\" , sep=\",\" ,header=True, inferSchema=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3101777f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|            Industry|\n",
      "+-------+--------------------+\n",
      "|  count|                  43|\n",
      "|   mean|                NULL|\n",
      "| stddev|                NULL|\n",
      "|    min|  Accounting/Finance|\n",
      "|    max|Transportation/Lo...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 6 Calculate basic summary statistics for a specific column in the DataFrame.\n",
    "df.describe().show() # workds kinda like pandas\n",
    "df.select(\"Industry\").describe().show() # for specific columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
